{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9a324b",
   "metadata": {},
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b08562",
   "metadata": {},
   "source": [
    "ans.Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design and train better features.\n",
    "It helps to eliminate the sitiuation of \"garbage in and garbage out\".\n",
    "\n",
    "Various aspects of feature engineering:\n",
    "\n",
    "1.Imputation\n",
    "\n",
    "2.Discretization\n",
    "\n",
    "3.Categorical Encoding\n",
    "\n",
    "4.Feature Splitting\n",
    "\n",
    "5.Handling Outliers\n",
    "\n",
    "6.Variable Transformations\n",
    "\n",
    "7.Scaling\n",
    "\n",
    "8.Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f20ff7",
   "metadata": {},
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcae0c",
   "metadata": {},
   "source": [
    "ans.Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "There are two main types of feature selection techniques: supervised and unsupervised, and supervised methods may be divided into wrapper, filter and intrinsic.\n",
    "Filter-based feature selection methods use statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features.\n",
    "Statistical measures for feature selection must be carefully chosen based on the data type of the input variable and the output or response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ce893",
   "metadata": {},
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8c1d4",
   "metadata": {},
   "source": [
    "ans.In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "One of the best ways for implementing feature selection with wrapper methods is to use Boruta package that finds the importance of a feature by creating shadow features.\n",
    "\n",
    "It works in the following steps:\n",
    "\n",
    "Firstly, it adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features).\n",
    "\n",
    "Then, it trains a random forest classifier on the extended data set and applies a feature importance measure (the default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important.\n",
    "\n",
    "At every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z-score than the maximum Z-score of its shadow features) and constantly removes features which are deemed highly unimportant.\n",
    "\n",
    "Finally, the algorithm stops either when all features get confirmed or rejected or it reaches a specified limit of random forest runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bbcdc",
   "metadata": {},
   "source": [
    "# 4.i. Describe the overall feature selection process.\n",
    "# ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae653c",
   "metadata": {},
   "source": [
    " Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd350f",
   "metadata": {},
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da7371",
   "metadata": {},
   "source": [
    " Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3ef3c",
   "metadata": {},
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8de56",
   "metadata": {},
   "source": [
    "ans.Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce5295",
   "metadata": {},
   "source": [
    "# 7. i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "# ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f036da",
   "metadata": {},
   "source": [
    "ans. Thus the Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors 01101010 and 11011011. They differ in four places, so the Hamming distance d(01101010,11011011) = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad1b7b",
   "metadata": {},
   "source": [
    "# 8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "Ans: High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd682552",
   "metadata": {},
   "source": [
    "# 9. Make a few quick notes on:\n",
    "\n",
    "# 1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "# 2. Use of vectors\n",
    "\n",
    "# 3. Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9138a7",
   "metadata": {},
   "source": [
    "Ans: The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1d755",
   "metadata": {},
   "source": [
    "# 10. Make a comparison between:\n",
    "# 1.Sequential backward exclusion vs. sequential forward selection\n",
    "# 2.Function selection methods: filter vs. wrapper\n",
    "# 3.SMC vs. Jaccard coefficient\n",
    "\n",
    "Ans: Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "The Jaccard coefficient is a measure of the percentage of overlap between sets defined as: (5.1) where W1 and W2 are two sets, in our case the 1-year windows of the ego networks. The Jaccard coefficient can be a value between 0 and 1, with 0 indicating no overlap and 1 complete overlap between the sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
